{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "conda 4.12.0 requires ruamel_yaml_conda>=0.11.14, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.13 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading selenium-4.7.2-py3-none-any.whl (6.3 MB)\n",
      "Collecting certifi>=2021.10.8\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "Collecting urllib3[socks]~=1.26\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\sweth\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sweth\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.2.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\sweth\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9; python_version < \"3.11\"\n",
      "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\" in c:\\users\\sweth\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.3)\n",
      "Requirement already satisfied: idna in c:\\users\\sweth\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6; extra == \"socks\" in c:\\users\\sweth\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sweth\\anaconda3\\lib\\site-packages (from cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\"->trio~=0.17->selenium) (2.20)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: certifi, sniffio, exceptiongroup, outcome, trio, urllib3, h11, wsproto, trio-websocket, selenium\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2020.6.20\n",
      "    Uninstalling certifi-2020.6.20:\n",
      "      Successfully uninstalled certifi-2020.6.20\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "Successfully installed certifi-2022.12.7 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.7.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.13 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from selenium import webdriver\n",
    "\n",
    "###########################\n",
    "# Loop of all the pages\n",
    "###########################\n",
    "\n",
    "#Loop to go over all pages\n",
    "pages = np.arange(1, 3, 1)\n",
    "data=[]\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    page=\"https://www.hostelworld.com/s?q=Barcelona,%20Catalonia,%20Spain&country=Spain&city=\\\n",
    "            Barcelona&type=city&id=83&from=2020-07-03&to=2020-07-05&guests=1&page=\" + str(page) \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(page)  \n",
    "    sleep(randint(2,10))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    my_table = soup.find_all(class_=['description', 'price-label body-3','price title-5',\\\n",
    "                                    'score orange big'])\n",
    "\n",
    "    for tag in my_table:\n",
    "        data.append(tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################\n",
    "#First loop: getting the URLs \n",
    "##############################\n",
    "\n",
    "pages = np.arange(1, 3, 1)\n",
    "url_collected=[]\n",
    "\n",
    "for page in pages:\n",
    "    page=\"https://www.hostelworld.com/s?q=Barcelona,%20Catalonia,%20Spain&country=Spain&city=Barcelona&type=city&id=83&from=2020-07-03&to=2020-07-05&guests=1&page=\" + str(page) \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(page)  \n",
    "    sleep(randint(5,15))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    urls = [item.get(\"href\") for item in soup.find_all(\"a\")]\n",
    "    \n",
    "    \n",
    "#Remove duplicates and none values\n",
    "urls_final = list(dict.fromkeys(url_collected))\n",
    "urls_final = list(filter(None, urls_final)) \n",
    "\n",
    "#Remove if not starting with pwa, remove if ending with display=reviews\n",
    "url_final = [x for x in urls_final if x.startswith('/pwa/')]\n",
    "url_final = [x for x in url_final if not x.endswith('display=reviews')]\n",
    "   \n",
    "string = 'https://www.hostelworld.com'\n",
    "final_list=[string + s for s in url_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-0407b9da1cd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdriver2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdriver2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "\n",
    "for i in range(0,10):\n",
    "    urls = final_list[i]\n",
    "    driver2 = webdriver.Chrome()\n",
    "    driver2.get(url)  \n",
    "    sleep(randint(10,20))\n",
    "    soup = BeautifulSoup(driver2.page_source, 'html.parser')\n",
    "    my_table2 = soup.find_all(class_=['title-2', 'rating-score body-3'])\n",
    "    \n",
    "    review=soup.find_all(class_='reviews')[-1]\n",
    "    \n",
    "    try:\n",
    "        price=soup.find_all('span', attrs={'class':'price'})[-1] \n",
    "    except:\n",
    "        price=soup.find_all('span', attrs={'class':'price'})\n",
    "\n",
    "    for tag in my_table2:\n",
    "        data.append(tag.text.strip())\n",
    "        \n",
    "    for p in price:\n",
    "        data.append(p)\n",
    "        \n",
    "    for r in review:\n",
    "        data.append(r)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
